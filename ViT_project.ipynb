{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN42zYmEnw6AyCn3EtzT1tv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29edfc93239d406e95a3da2db64624f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ac12503e90e407bb5a9e4624456d931",
              "IPY_MODEL_7ecab4d44c2f4ce3b49a6b080751de69",
              "IPY_MODEL_70b0e96a3dfe40eb9d9789ed423246ef"
            ],
            "layout": "IPY_MODEL_365beeec3b6d41049cfe38e441d7de6f",
            "tabbable": null,
            "tooltip": null
          }
        },
        "6ac12503e90e407bb5a9e4624456d931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b49b5a9c7edf47ed8c2b153aa75ff862",
            "placeholder": "​",
            "style": "IPY_MODEL_9d24fc263d3f4ec7bba577e35a9d1cb0",
            "tabbable": null,
            "tooltip": null,
            "value": "Resolving data files: 100%"
          }
        },
        "7ecab4d44c2f4ce3b49a6b080751de69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_bb263f16c75d4002a2b39a90d6313b7e",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b53fe3c76ae84f3fa58fe089f665eb55",
            "tabbable": null,
            "tooltip": null,
            "value": 100000
          }
        },
        "70b0e96a3dfe40eb9d9789ed423246ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_8d0b7ce792b54df0833026fa693ee546",
            "placeholder": "​",
            "style": "IPY_MODEL_08ea6306e34845b5907a542739bdbec7",
            "tabbable": null,
            "tooltip": null,
            "value": " 100000/100000 [00:00&lt;00:00, 50525.15it/s]"
          }
        },
        "365beeec3b6d41049cfe38e441d7de6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b49b5a9c7edf47ed8c2b153aa75ff862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d24fc263d3f4ec7bba577e35a9d1cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "bb263f16c75d4002a2b39a90d6313b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53fe3c76ae84f3fa58fe089f665eb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d0b7ce792b54df0833026fa693ee546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ea6306e34845b5907a542739bdbec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "c33a29f4048b43738268915e57254450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_082074ea8bc246078633084c64bb2ab2",
              "IPY_MODEL_571fe119e0fd4fbdb530b5f71ae5fc01",
              "IPY_MODEL_30af967379594e2f826c372fcdc2dd4e"
            ],
            "layout": "IPY_MODEL_d81297c7374f4de2b1d0f710b1045664",
            "tabbable": null,
            "tooltip": null
          }
        },
        "082074ea8bc246078633084c64bb2ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1896c4b3845d4e549c257776569f535a",
            "placeholder": "​",
            "style": "IPY_MODEL_da6eea0c74c54efbaece76de6e82ed42",
            "tabbable": null,
            "tooltip": null,
            "value": "Generating train split: "
          }
        },
        "571fe119e0fd4fbdb530b5f71ae5fc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_6ca25b456519423594ce64562c1c285a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_136016af7f0b4a328117c2d82b2a4a0a",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "30af967379594e2f826c372fcdc2dd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1ca11877efc3453c9722475fb4c57d1a",
            "placeholder": "​",
            "style": "IPY_MODEL_5049322b56454739bc1e6ae3e333617f",
            "tabbable": null,
            "tooltip": null,
            "value": " 100000/0 [00:04&lt;00:00, 21642.58 examples/s]"
          }
        },
        "d81297c7374f4de2b1d0f710b1045664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1896c4b3845d4e549c257776569f535a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da6eea0c74c54efbaece76de6e82ed42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "6ca25b456519423594ce64562c1c285a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "136016af7f0b4a328117c2d82b2a4a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ca11877efc3453c9722475fb4c57d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5049322b56454739bc1e6ae3e333617f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "087f54183e3d455fbecb666a1d1d2791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_707d28b3a0954860b1821cea0a7b7510",
              "IPY_MODEL_c5856aa6887a448db894fc59941e65bb",
              "IPY_MODEL_b681f186b3a64b93857aaeb0e03486dd"
            ],
            "layout": "IPY_MODEL_9f054e5338394dd9b09f07a9d73dbff5",
            "tabbable": null,
            "tooltip": null
          }
        },
        "707d28b3a0954860b1821cea0a7b7510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b36dfc5e526a4d73b51f1f0466544192",
            "placeholder": "​",
            "style": "IPY_MODEL_7c2e3eadfc0e4ca49708cf554322c0a6",
            "tabbable": null,
            "tooltip": null,
            "value": "Resolving data files: 100%"
          }
        },
        "c5856aa6887a448db894fc59941e65bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_026808b2c3764fe4ad58d9b69fe7e123",
            "max": 7180,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99263ef5ab5445a48881c6be9c7b1e96",
            "tabbable": null,
            "tooltip": null,
            "value": 7180
          }
        },
        "b681f186b3a64b93857aaeb0e03486dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b43351df9b4f4423ad665afbd812fb53",
            "placeholder": "​",
            "style": "IPY_MODEL_5b91ce2fd0ec45be971b3dc910e0a896",
            "tabbable": null,
            "tooltip": null,
            "value": " 7180/7180 [00:00&lt;00:00, 81693.55it/s]"
          }
        },
        "9f054e5338394dd9b09f07a9d73dbff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36dfc5e526a4d73b51f1f0466544192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c2e3eadfc0e4ca49708cf554322c0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "026808b2c3764fe4ad58d9b69fe7e123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99263ef5ab5445a48881c6be9c7b1e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b43351df9b4f4423ad665afbd812fb53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b91ce2fd0ec45be971b3dc910e0a896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "002f874bab8d4b109bd10717b29f1956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2bcd5697e6f4635ac4d6022bcf00ebf",
              "IPY_MODEL_ec9ff6a986324fe2993d100b254d40f9",
              "IPY_MODEL_531e232971cf4f4d8404ddf86583ed01"
            ],
            "layout": "IPY_MODEL_f0d9730add6b4172bdfdf50880c7d7ee",
            "tabbable": null,
            "tooltip": null
          }
        },
        "f2bcd5697e6f4635ac4d6022bcf00ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_a20a8b67111d49aaad7f0b2cca3ba979",
            "placeholder": "​",
            "style": "IPY_MODEL_8d5bf3073e59416f94161166fa58b4f9",
            "tabbable": null,
            "tooltip": null,
            "value": "Generating train split: "
          }
        },
        "ec9ff6a986324fe2993d100b254d40f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_00d20cf24fd74347b240e79b289d2b2b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8811ba5e28c241b0b6393be4d0f683b9",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "531e232971cf4f4d8404ddf86583ed01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_371f2d4dd80c44108e01368703b17c90",
            "placeholder": "​",
            "style": "IPY_MODEL_c0faff2b2345442bb3429b0712b51875",
            "tabbable": null,
            "tooltip": null,
            "value": " 7180/0 [00:00&lt;00:00, 20713.19 examples/s]"
          }
        },
        "f0d9730add6b4172bdfdf50880c7d7ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a20a8b67111d49aaad7f0b2cca3ba979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d5bf3073e59416f94161166fa58b4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "00d20cf24fd74347b240e79b289d2b2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8811ba5e28c241b0b6393be4d0f683b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "371f2d4dd80c44108e01368703b17c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0faff2b2345442bb3429b0712b51875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kazi-Rakib-Hasan-Jawwad/Histo-FSL/blob/master/ViT_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to connect Colab with local runtime:\n",
        "1. Put this command in virtual environment terminal:\n",
        "\n",
        "> jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n",
        "\n",
        "2. Copy and paste the url in colab.\n",
        "\n",
        "\n",
        "Check availability of GPU."
      ],
      "metadata": {
        "id": "QbGpN-OoBA7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WolzVH9Y24Db",
        "outputId": "b7c173d1-0d0e-4a5e-95cb-9ee03f486c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__CUDNN VERSION: 8500\n",
            "__Number CUDA Devices: 1\n",
            "__CUDA Device Name: NVIDIA GeForce RTX 3080 Ti\n",
            "__CUDA Device Total Memory [GB]: 12.636192768\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
        "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PnBq8NhvC1UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import tqdm\n",
        "from torchvision import transforms\n",
        "from torch import Tensor, nn\n",
        "from abc import abstractmethod\n",
        "from typing import Optional\n",
        "from transformers import ViTModel\n",
        "from easyfsl.datasets import FeaturesDataset\n",
        "from easyfsl.samplers import TaskSampler\n",
        "from easyfsl.methods import PrototypicalNetworks, RelationNetworks, SimpleShot, BDCSPN, TIM, PTMAP\n",
        "from easyfsl.utils import evaluate\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "pyIvh7DgpTr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from pathlib import Path\n",
        "working_directory = Path(\"/home/rakib/jupyter_notebooks/iBOT_project\")\n",
        "cache_dir = working_directory / \"cache\"\n",
        "'''"
      ],
      "metadata": {
        "id": "xTl9SNStCyjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model: torch.nn) -> None:\n",
        "    \"\"\"Print number of trainable parameters.\"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param}\"\n",
        "        f\" || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "l2-TVRkrpgxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_embeddings_vit(\n",
        "    dataloader: DataLoader,\n",
        "    model: nn.Module,\n",
        "    device: Optional[str] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Predict embeddings for a dataloader.\n",
        "    Args:\n",
        "        dataloader: dataloader to predict embeddings for. Must deliver tuples (images, class_names)\n",
        "        model: model to use for prediction\n",
        "        device: device to cast the images to. If none, no casting is performed. Must be the same as\n",
        "            the device the model is on.\n",
        "    Returns:\n",
        "        dataframe with columns embedding and class_name\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    all_class_names = []\n",
        "    with torch.no_grad():\n",
        "        for images, class_names in tqdm(\n",
        "            dataloader, unit=\"batch\", desc=\"Predicting embeddings\"\n",
        "        ):\n",
        "            if device is not None:\n",
        "                images = images.to(device)\n",
        "            all_embeddings.append(model(images).last_hidden_state[:, 0, :].detach().cpu())  # Changed from: all_embeddings.append(model(images).detach().cpu())\n",
        "            if isinstance(class_names, torch.Tensor):\n",
        "                all_class_names += class_names.tolist()\n",
        "            else:\n",
        "                all_class_names += class_names\n",
        "\n",
        "    concatenated_embeddings = torch.cat(all_embeddings)\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        {\"embedding\": list(concatenated_embeddings), \"class_name\": all_class_names}\n",
        "    )"
      ],
      "metadata": {
        "id": "AvyrVcZZBzS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_prototypes(support_features: Tensor, support_labels: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute class prototypes from support features and labels\n",
        "    Args:\n",
        "        support_features: for each instance in the support set, its feature vector\n",
        "        support_labels: for each instance in the support set, its label\n",
        "\n",
        "    Returns:\n",
        "        for each label of the support set, the average feature vector of instances with this label\n",
        "    \"\"\"\n",
        "\n",
        "    n_way = len(torch.unique(support_labels))\n",
        "    # Prototype i is the mean of all instances of features corresponding to labels == i\n",
        "    return torch.cat(\n",
        "        [\n",
        "            support_features[torch.nonzero(support_labels == label)].mean(0)\n",
        "            for label in range(n_way)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "class FewShotClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_dim: int,\n",
        "        num_classes: int,\n",
        "        use_softmax: bool = False,\n",
        "        feature_centering: Optional[Tensor] = None,\n",
        "        feature_normalization: Optional[float] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_layer = nn.Sequential(nn.Linear(feature_dim, 128), nn.Linear(128, num_classes))\n",
        "        #self.conv_layer = nn.Conv2d(in_channels, num_classes, kernel_size=3, padding=1)\n",
        "        self.use_softmax = use_softmax\n",
        "\n",
        "        self.prototypes = torch.tensor(())\n",
        "\n",
        "        self.feature_centering = (\n",
        "            feature_centering if feature_centering is not None else torch.tensor(0)\n",
        "        )\n",
        "        self.feature_normalization = feature_normalization\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(\n",
        "        self,\n",
        "        query_images: Tensor,\n",
        "    ) -> Tensor:\n",
        "        raise NotImplementedError(\"All few-shot algorithms must implement a forward method.\")\n",
        "\n",
        "    def process_support_set(\n",
        "        self,\n",
        "        support_images: Tensor,\n",
        "        support_labels: Tensor,\n",
        "    ):\n",
        "        self.compute_prototypes_and_store_support_set(support_images, support_labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def is_transductive() -> bool:\n",
        "        raise NotImplementedError(\"All few-shot algorithms must implement an is_transductive method.\")\n",
        "\n",
        "    def compute_features(self, images: Tensor) -> Tensor:\n",
        "        original_features = self.backbone(images)\n",
        "        centered_features = original_features - self.feature_centering\n",
        "        if self.feature_normalization is not None:\n",
        "            return nn.functional.normalize(centered_features, p=self.feature_normalization, dim=1)\n",
        "        return centered_features\n",
        "\n",
        "    def softmax_if_specified(self, output: Tensor, temperature: float = 1.0) -> Tensor:\n",
        "        return (temperature * output).softmax(-1) if self.use_softmax else output\n",
        "\n",
        "    def compute_prototypes_and_store_support_set(\n",
        "        self,\n",
        "        support_images: Tensor,\n",
        "        support_labels: Tensor,\n",
        "    ):\n",
        "        self.support_labels = support_labels\n",
        "        self.support_features = self.compute_features(support_images)\n",
        "        self._raise_error_if_features_are_multi_dimensional(self.support_features)\n",
        "        self.prototypes = compute_prototypes(self.support_features, support_labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def _raise_error_if_features_are_multi_dimensional(features: Tensor):\n",
        "        if len(features.shape) != 2:\n",
        "            raise ValueError(\n",
        "                \"Illegal backbone or feature shape. \"\n",
        "                \"Expected output for an image is a 1-dim tensor.\"\n",
        "            )\n",
        "\n",
        "class ModifiedPrototypicalNetworks(FewShotClassifier):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_dim: int,\n",
        "        num_classes: int,\n",
        "        use_softmax: bool = False,\n",
        "        feature_centering: Optional[Tensor] = None,\n",
        "        feature_normalization: Optional[float] = None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            feature_dim,\n",
        "            num_classes,\n",
        "            use_softmax=use_softmax,\n",
        "            feature_centering=feature_centering,\n",
        "            feature_normalization=feature_normalization,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query_images: Tensor,\n",
        "    ) -> Tensor:\n",
        "        query_features = self.compute_features(query_images)\n",
        "        self._raise_error_if_features_are_multi_dimensional(query_features)\n",
        "\n",
        "        scores = self.linear_layer(query_features)\n",
        "\n",
        "        return self.softmax_if_specified(scores)\n"
      ],
      "metadata": {
        "id": "8HdO3OlScBeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to see what functions or models are available in module class. I can call it by import module\n",
        "\n",
        "import module\n",
        "print(dir(module))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwlBZioOu1df",
        "outputId": "240048ad-29c5-40d9-a7fc-88b08471e9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chowder', 'ExtremeLayer', 'List', 'MLP', 'MaskedLinear', 'Optional', 'TilesMLP', 'Tuple', 'Union', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'nn', 'torch', 'warnings']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from module import Chowder\n",
        "\n",
        "chowder = Chowder(\n",
        "    in_features=768,                     # output dimension of Phikon\n",
        "    out_features=1,                      # dimension of predictions (a probability for class \"1\")\n",
        "    n_top=5,                             # number of top scores in Chowder (in the image, N is 2)\n",
        "    n_bottom=5,                          # number of bottom scores in Chowder\n",
        "    mlp_hidden=[200, 100],               # MLP hidden layers after the max-min layer\n",
        "    mlp_activation=torch.nn.Sigmoid(),   # MLP activation\n",
        "    bias=True                            # bias for first 1D convolution which computes scores\n",
        ")\n",
        "\n",
        "# Chowder has 23,170 parameters: it's a very small model !\n",
        "print_trainable_parameters(chowder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmEJ6MvkMFhT",
        "outputId": "7726e503-0cb8-4773-e19a-77963ebe94fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 23170 || all params: 23170 || trainable%: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load phikon\n",
        "\n",
        "model = ViTModel.from_pretrained(\"owkin/phikon\", add_pooling_layer=False)\n",
        "model.to(device)\n",
        "#model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGkWF4mZh3DJ",
        "outputId": "43036986-ef06-476a-c8b2-9fa8569c59f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTModel(\n",
              "  (embeddings): ViTEmbeddings(\n",
              "    (patch_embeddings): ViTPatchEmbeddings(\n",
              "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (encoder): ViTEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (1): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (2): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (3): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (4): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (5): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (6): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (7): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (8): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (9): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (10): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (11): ViTLayer(\n",
              "        (attention): ViTAttention(\n",
              "          (attention): ViTSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory_k = Path(\"/home/rakib/models/paper_benchmarking_ssl_diverse_pathology/inference/kather/ibot/2\")\n",
        "\n",
        "data_to_test = Path(\"/home/rakib/data/kather_texture/\")\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor()])\n",
        "testset = torchvision.datasets.ImageFolder(data_to_test, transform=transform)\n",
        "\n",
        "classes = testset.classes\n",
        "print(classes)\n",
        "dataloader = DataLoader(testset, batch_size=128, shuffle=False, drop_last=False)\n",
        "feature_df = predict_embeddings_vit(dataloader, model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOhsMI6rhl9B",
        "outputId": "1c168ce2-1f19-41c3-90db-320533f97216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adipose', 'complex', 'debris', 'empty', 'lympho', 'mucosa', 'stroma', 'tumor']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting embeddings: 100%|█████████████████| 40/40 [00:24<00:00,  1.64batch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = ModifiedPrototypicalNetworks(feature_dim=728, num_classes=1)\n",
        "print_trainable_parameters(model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og3Hl4EjcEsf",
        "outputId": "26dc26ca-69f0-459f-acdb-b705ef0e286d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 93441 || all params: 93441 || trainable%: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TaskSampler for FSL\n",
        "task_sampler = TaskSampler(\n",
        "    feature_df,\n",
        "    n_way=3,\n",
        "    n_shot=5,\n",
        "    n_query=10,\n",
        "    n_tasks=100,\n",
        ")\n",
        "\n",
        "# DataLoader using TaskSampler\n",
        "features_loader = DataLoader(\n",
        "    feature_df,\n",
        "    batch_sampler=task_sampler,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    collate_fn=task_sampler.episodic_collate_fn,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "dbvSxqJLvMOm",
        "outputId": "6058b14f-6fa1-46a5-f722-94627158ee16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'get_labels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_150860/4203486929.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;31m# TaskSampler for FSL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m task_sampler = TaskSampler(\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeature_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_way\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/project-6/lib/python3.8/site-packages/easyfsl/samplers/task_sampler.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dataset, n_way, n_shot, n_query, n_tasks)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems_per_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems_per_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems_per_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/project-6/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'get_labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "from pathlib import Path\n",
        "import copy\n",
        "\n",
        "\n",
        "# Define your loss function and other parameters\n",
        "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
        "n_epochs = 200\n",
        "scheduler_milestones = [120, 160]\n",
        "scheduler_gamma = 0.1\n",
        "learning_rate = 1e-5\n",
        "tb_logs_dir = Path(\".\")\n",
        "train_optimizer = SGD(\n",
        "    few_shot_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-5\n",
        ")\n",
        "train_scheduler = MultiStepLR(\n",
        "    train_optimizer,\n",
        "    milestones=scheduler_milestones,\n",
        "    gamma=scheduler_gamma,\n",
        ")\n",
        "tb_writer = SummaryWriter(log_dir=str(tb_logs_dir))\n",
        "\n",
        "# Define your custom training epoch function\n",
        "def training_epoch(\n",
        "    model: FewShotClassifier, data_loader: DataLoader, optimizer: Optimizer\n",
        "):\n",
        "    all_loss = []\n",
        "    model.train()\n",
        "    with tqdm(\n",
        "        enumerate(kather_emb_df), total=len(kather_emb_df), desc=\"Training\"\n",
        "    ) as tqdm_train:\n",
        "        for episode_index, (\n",
        "            support_images,\n",
        "            support_labels,\n",
        "            query_images,\n",
        "            query_labels,\n",
        "            _,\n",
        "        ) in tqdm_train:\n",
        "            optimizer.zero_grad()\n",
        "            model.process_support_set(\n",
        "                support_images.to(DEVICE), support_labels.to(DEVICE)\n",
        "            )\n",
        "            classification_scores = model(query_images.to(DEVICE))\n",
        "            loss = LOSS_FUNCTION(classification_scores, query_labels.to(DEVICE))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            all_loss.append(loss.item())\n",
        "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
        "    return mean(all_loss)\n",
        "\n",
        "# Initialize variables for best state and best validation accuracy\n",
        "best_state = few_shot_classifier.state_dict()\n",
        "best_validation_accuracy = 0.0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    average_loss = training_epoch(few_shot_classifier, train_loader, train_optimizer)\n",
        "    validation_accuracy = evaluate(\n",
        "        few_shot_classifier, val_loader, device=DEVICE, tqdm_prefix=\"Validation\"\n",
        "    )\n",
        "\n",
        "    # Update best state if validation accuracy improves\n",
        "    if validation_accuracy > best_validation_accuracy:\n",
        "        best_validation_accuracy = validation_accuracy\n",
        "        best_state = copy.deepcopy(few_shot_classifier.state_dict())\n",
        "        print(\"Ding ding ding! We found a new best model!\")\n",
        "\n",
        "    # Log metrics to tensorboard\n",
        "    tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
        "    tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    train_scheduler.step()\n",
        "\n",
        "# Load the best state\n",
        "few_shot_classifier.load_state_dict(best_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "g4LjtL1-qlum",
        "outputId": "1faca1e8-872f-4186-8241-9fc1ddf8735a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[51], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# TaskSampler for FSL\u001b[39;00m\n\u001b[1;32m     11\u001b[0m task_sampler \u001b[38;5;241m=\u001b[39m TaskSampler(\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mfeatures_dataset\u001b[49m,\n\u001b[1;32m     13\u001b[0m     n_way\u001b[38;5;241m=\u001b[39mn_way,\n\u001b[1;32m     14\u001b[0m     n_shot\u001b[38;5;241m=\u001b[39mn_shots,\n\u001b[1;32m     15\u001b[0m     n_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     16\u001b[0m     n_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# DataLoader using TaskSampler\u001b[39;00m\n\u001b[1;32m     20\u001b[0m features_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     21\u001b[0m     features_dataset,\n\u001b[1;32m     22\u001b[0m     batch_sampler\u001b[38;5;241m=\u001b[39mtask_sampler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mtask_sampler\u001b[38;5;241m.\u001b[39mepisodic_collate_fn,\n\u001b[1;32m     26\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import auc, pad_collate_fn\n",
        "import utils\n",
        "print(utils.__file__)\n",
        "\n",
        "# We define the loss function, optimizer and metrics for the training\n",
        "criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = torch.optim.Adam              # Adam optimizer\n",
        "metrics = {\"auc\": auc}                    # AUC will be the tracking metric\n",
        "\n",
        "# ``collator`` is a function that apply a deterministic\n",
        "# transformation to a batch of samples before being processed\n",
        "# by the GPU. Here, this function is ``pad_collate_fn``. The\n",
        "# goal of this function is align matrices of features (the inputs)\n",
        "# in terms of shape. Indeed, some WSI may have 200 features (very\n",
        "# small piece of tissues) or 1,000 (the maximum we set). In that case,\n",
        "# all matrices will have a shape of at most the bigger matrices in the\n",
        "# batch. Our (200, 768) input matrix will become a (1000, 768) matrix\n",
        "# with 800 ``inf`` values. A boolean mask is stored so that to tell\n",
        "# torch not to process these 800 values but only focus on the 200 real ones\n",
        "\n",
        "collator = pad_collate_fn\n"
      ],
      "metadata": {
        "id": "u5Ja04FeMUoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b7b06c-0636-4c02-8d1e-34b7dde23189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/rakib/jupyter_notebooks/iBOT_project/cache/datasets--owkin--camelyon16-features/snapshots/932e3f46255585b9a83cd3f0d74bf1c806fea5a0/scripts/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from copy import deepcopy\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from trainer import TorchTrainer, slide_level_train_step, slide_level_val_step\n",
        "\n",
        "# We run a 5-fold cross-validation with 1 repeat (you can tweak these parameters)\n",
        "n_repeats = 1\n",
        "n_folds = 5\n",
        "train_metrics, val_metrics = [], []\n",
        "test_logits = []\n",
        "\n",
        "cv_start_time = datetime.now()\n",
        "\n",
        "for repeat in range(n_repeats):\n",
        "    print(f\"Running cross-validation #{repeat+1}\")\n",
        "    # We stratify with respect to the training labels\n",
        "    cv_skfold = StratifiedKFold(\n",
        "        n_splits=n_folds,\n",
        "        shuffle=True,\n",
        "        random_state=repeat,\n",
        "    )\n",
        "    cv_splits = cv_skfold.split(cam16_design_indices, y=cam16_design_labels)\n",
        "\n",
        "    # 1 training fold approximately takes 25 seconds\n",
        "    for i, (train_indices, val_indices) in enumerate(cv_splits):\n",
        "        fold_start_time = datetime.now()\n",
        "        trainer = TorchTrainer(\n",
        "            model=deepcopy(chowder),\n",
        "            criterion=criterion,\n",
        "            metrics=metrics,\n",
        "            batch_size=16,                           # you can tweak this\n",
        "            num_epochs=15,                           # you can tweak this\n",
        "            learning_rate=1e-3,                      # you can tweak this\n",
        "            weight_decay=0.0,                        # you can tweak this\n",
        "            device=\"cuda:0\",\n",
        "            num_workers=multiprocessing.cpu_count(), # you can tweak this\n",
        "            optimizer=deepcopy(optimizer),\n",
        "            train_step=slide_level_train_step,\n",
        "            val_step=slide_level_val_step,\n",
        "            collator=pad_collate_fn,\n",
        "        )\n",
        "\n",
        "        print(f\"Running cross-validation on split #{i+1}\")\n",
        "        cam16_train_dataset = torch.utils.data.Subset(\n",
        "            cam16_design_dataset, indices=train_indices\n",
        "        )\n",
        "        cam16_val_dataset = torch.utils.data.Subset(\n",
        "            cam16_design_dataset, indices=val_indices\n",
        "        )\n",
        "\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "            # Training step for the given number of epochs\n",
        "            local_train_metrics, local_val_metrics = trainer.train(\n",
        "                cam16_train_dataset, cam16_val_dataset\n",
        "            )\n",
        "            # Predictions on test (logits, sigmoid(logits) = probability)\n",
        "            local_test_logits = trainer.predict(cam16_test_dataset)[1]\n",
        "\n",
        "        train_metrics.append(local_train_metrics)\n",
        "        val_metrics.append(local_val_metrics)\n",
        "        test_logits.append(local_test_logits)\n",
        "        fold_end_time = datetime.now()\n",
        "        fold_running_time = fold_end_time - fold_start_time\n",
        "        print(\"\\n-----------------------------Finished in {}---------------------------------------\\n\".format(fold_running_time))\n",
        "    #clear_output()\n",
        "cv_end_time = datetime.now()\n",
        "cv_running_time = cv_end_time - cv_start_time\n",
        "print(\"\\nFinished cross-validation in {}\".format(cv_running_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PXi6sncMcAT",
        "outputId": "f3df341f-9d49-444e-b15d-1cbf926e3b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running cross-validation #1\n",
            "Running cross-validation on split #1\n",
            "Epoch 1: train_loss=0.69553, train_auc=0.5032, val_loss=0.68943, val_auc=0.4687\n",
            "Epoch 2: train_loss=0.67532, train_auc=0.5332, val_loss=0.64351, val_auc=0.4901\n",
            "Epoch 3: train_loss=0.69388, train_auc=0.4789, val_loss=0.67824, val_auc=0.4957\n",
            "Epoch 4: train_loss=0.67432, train_auc=0.5521, val_loss=0.64426, val_auc=0.4872\n",
            "Epoch 5: train_loss=0.68850, train_auc=0.4389, val_loss=0.65221, val_auc=0.4943\n",
            "Epoch 6: train_loss=0.68018, train_auc=0.5465, val_loss=0.65567, val_auc=0.6037\n",
            "Epoch 7: train_loss=0.67314, train_auc=0.6061, val_loss=0.61848, val_auc=0.8196\n",
            "Epoch 8: train_loss=0.61496, train_auc=0.8317, val_loss=0.56281, val_auc=0.9219\n",
            "Epoch 9: train_loss=0.50494, train_auc=0.9342, val_loss=0.42341, val_auc=0.9602\n",
            "Epoch 10: train_loss=0.39363, train_auc=0.9394, val_loss=0.31809, val_auc=0.9673\n",
            "Epoch 11: train_loss=0.30270, train_auc=0.9530, val_loss=0.28298, val_auc=0.9716\n",
            "Epoch 12: train_loss=0.25982, train_auc=0.9657, val_loss=0.23114, val_auc=0.9730\n",
            "Epoch 13: train_loss=0.26863, train_auc=0.9564, val_loss=0.26405, val_auc=0.9744\n",
            "Epoch 14: train_loss=0.22108, train_auc=0.9713, val_loss=0.24298, val_auc=0.9787\n",
            "Epoch 15: train_loss=0.20713, train_auc=0.9710, val_loss=0.19300, val_auc=0.9801\n",
            "\n",
            "-----------------------------Finished in 0:00:36.587066---------------------------------------\n",
            "\n",
            "Running cross-validation on split #2\n",
            "Epoch 1: train_loss=0.75023, train_auc=0.4372, val_loss=0.67819, val_auc=0.4730\n",
            "Epoch 2: train_loss=0.71019, train_auc=0.4870, val_loss=0.64481, val_auc=0.4602\n",
            "Epoch 3: train_loss=0.68971, train_auc=0.5002, val_loss=0.69295, val_auc=0.5014\n",
            "Epoch 4: train_loss=0.68948, train_auc=0.4781, val_loss=0.64785, val_auc=0.4787\n",
            "Epoch 5: train_loss=0.68276, train_auc=0.4759, val_loss=0.65917, val_auc=0.4474\n",
            "Epoch 6: train_loss=0.69029, train_auc=0.4809, val_loss=0.65893, val_auc=0.4631\n",
            "Epoch 7: train_loss=0.69674, train_auc=0.4398, val_loss=0.65104, val_auc=0.4545\n",
            "Epoch 8: train_loss=0.66447, train_auc=0.6414, val_loss=0.65398, val_auc=0.4545\n",
            "Epoch 9: train_loss=0.68075, train_auc=0.5384, val_loss=0.66674, val_auc=0.4631\n",
            "Epoch 10: train_loss=0.64852, train_auc=0.7272, val_loss=0.66502, val_auc=0.4759\n",
            "Epoch 11: train_loss=0.61751, train_auc=0.7751, val_loss=0.64238, val_auc=0.5838\n",
            "Epoch 12: train_loss=0.55324, train_auc=0.8587, val_loss=0.56363, val_auc=0.7983\n",
            "Epoch 13: train_loss=0.44040, train_auc=0.9239, val_loss=0.45199, val_auc=0.8594\n",
            "Epoch 14: train_loss=0.30913, train_auc=0.9724, val_loss=0.41641, val_auc=0.8537\n",
            "Epoch 15: train_loss=0.25467, train_auc=0.9669, val_loss=0.40122, val_auc=0.8693\n",
            "\n",
            "-----------------------------Finished in 0:00:34.947935---------------------------------------\n",
            "\n",
            "Running cross-validation on split #3\n",
            "Epoch 1: train_loss=0.70939, train_auc=0.4926, val_loss=0.67581, val_auc=0.5952\n",
            "Epoch 2: train_loss=0.67684, train_auc=0.5345, val_loss=0.64360, val_auc=0.5185\n",
            "Epoch 3: train_loss=0.68974, train_auc=0.5000, val_loss=0.69057, val_auc=0.4957\n",
            "Epoch 4: train_loss=0.69192, train_auc=0.4731, val_loss=0.64344, val_auc=0.5582\n",
            "Epoch 5: train_loss=0.67303, train_auc=0.6030, val_loss=0.66890, val_auc=0.6207\n",
            "Epoch 6: train_loss=0.68284, train_auc=0.4778, val_loss=0.64797, val_auc=0.6065\n",
            "Epoch 7: train_loss=0.67587, train_auc=0.5904, val_loss=0.65181, val_auc=0.5810\n",
            "Epoch 8: train_loss=0.68055, train_auc=0.5356, val_loss=0.66979, val_auc=0.5895\n",
            "Epoch 9: train_loss=0.68282, train_auc=0.5436, val_loss=0.64895, val_auc=0.5966\n",
            "Epoch 10: train_loss=0.65625, train_auc=0.7088, val_loss=0.62964, val_auc=0.6903\n",
            "Epoch 11: train_loss=0.62894, train_auc=0.7554, val_loss=0.55896, val_auc=0.8594\n",
            "Epoch 12: train_loss=0.51855, train_auc=0.9100, val_loss=0.45971, val_auc=0.9318\n",
            "Epoch 13: train_loss=0.40422, train_auc=0.9296, val_loss=0.33001, val_auc=0.9744\n",
            "Epoch 14: train_loss=0.30639, train_auc=0.9566, val_loss=0.26067, val_auc=0.9759\n",
            "Epoch 15: train_loss=0.25650, train_auc=0.9719, val_loss=0.23125, val_auc=0.9702\n",
            "\n",
            "-----------------------------Finished in 0:00:35.223572---------------------------------------\n",
            "\n",
            "Running cross-validation on split #4\n",
            "Epoch 1: train_loss=0.72772, train_auc=0.4745, val_loss=0.68169, val_auc=0.6676\n",
            "Epoch 2: train_loss=0.68733, train_auc=0.4646, val_loss=0.64544, val_auc=0.6207\n",
            "Epoch 3: train_loss=0.69204, train_auc=0.4816, val_loss=0.68830, val_auc=0.6108\n",
            "Epoch 4: train_loss=0.67806, train_auc=0.5384, val_loss=0.64290, val_auc=0.5966\n",
            "Epoch 5: train_loss=0.67277, train_auc=0.6231, val_loss=0.65131, val_auc=0.6065\n",
            "Epoch 6: train_loss=0.67488, train_auc=0.5690, val_loss=0.65821, val_auc=0.6051\n",
            "Epoch 7: train_loss=0.67703, train_auc=0.5459, val_loss=0.64730, val_auc=0.5966\n",
            "Epoch 8: train_loss=0.67541, train_auc=0.5502, val_loss=0.65861, val_auc=0.6151\n",
            "Epoch 9: train_loss=0.67444, train_auc=0.5922, val_loss=0.64814, val_auc=0.6562\n",
            "Epoch 10: train_loss=0.65920, train_auc=0.7088, val_loss=0.63295, val_auc=0.6804\n",
            "Epoch 11: train_loss=0.61431, train_auc=0.7763, val_loss=0.58469, val_auc=0.7230\n",
            "Epoch 12: train_loss=0.53701, train_auc=0.8421, val_loss=0.48275, val_auc=0.8494\n",
            "Epoch 13: train_loss=0.41386, train_auc=0.9025, val_loss=0.35014, val_auc=0.9347\n",
            "Epoch 14: train_loss=0.30798, train_auc=0.9522, val_loss=0.30400, val_auc=0.9560\n",
            "Epoch 15: train_loss=0.26253, train_auc=0.9586, val_loss=0.24995, val_auc=0.9574\n",
            "\n",
            "-----------------------------Finished in 0:00:34.543631---------------------------------------\n",
            "\n",
            "Running cross-validation on split #5\n",
            "Epoch 1: train_loss=0.68993, train_auc=0.5076, val_loss=0.65954, val_auc=0.4912\n",
            "Epoch 2: train_loss=0.67790, train_auc=0.4827, val_loss=0.64920, val_auc=0.4604\n",
            "Epoch 3: train_loss=0.68307, train_auc=0.4507, val_loss=0.66470, val_auc=0.4765\n",
            "Epoch 4: train_loss=0.67964, train_auc=0.5190, val_loss=0.64670, val_auc=0.4927\n",
            "Epoch 5: train_loss=0.67693, train_auc=0.5742, val_loss=0.65081, val_auc=0.5513\n",
            "Epoch 6: train_loss=0.66634, train_auc=0.5827, val_loss=0.64376, val_auc=0.5674\n",
            "Epoch 7: train_loss=0.65325, train_auc=0.6575, val_loss=0.61721, val_auc=0.6935\n",
            "Epoch 8: train_loss=0.59346, train_auc=0.8239, val_loss=0.53267, val_auc=0.8695\n",
            "Epoch 9: train_loss=0.47707, train_auc=0.8899, val_loss=0.39148, val_auc=0.9282\n",
            "Epoch 10: train_loss=0.36015, train_auc=0.9343, val_loss=0.29368, val_auc=0.9487\n",
            "Epoch 11: train_loss=0.27573, train_auc=0.9616, val_loss=0.24534, val_auc=0.9560\n",
            "Epoch 12: train_loss=0.24639, train_auc=0.9676, val_loss=0.21734, val_auc=0.9575\n",
            "Epoch 13: train_loss=0.22662, train_auc=0.9710, val_loss=0.20155, val_auc=0.9663\n",
            "Epoch 14: train_loss=0.18867, train_auc=0.9832, val_loss=0.17695, val_auc=0.9780\n",
            "Epoch 15: train_loss=0.18800, train_auc=0.9792, val_loss=0.17258, val_auc=0.9765\n",
            "\n",
            "-----------------------------Finished in 0:00:35.700061---------------------------------------\n",
            "\n",
            "\n",
            "Finished cross-validation in 0:02:57.005428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from easyfsl.datasets import FeaturesDataset\n",
        "from easyfsl.samplers import TaskSampler\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# Define your loss function, optimizer, and other training parameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Set the number of repeats and folds for cross-validation\n",
        "n_repeats = 1\n",
        "n_folds = 5\n",
        "train_metrics, val_metrics = [], []\n",
        "\n",
        "cv_start_time = datetime.now()\n",
        "\n",
        "# Loop over repeats and folds\n",
        "for repeat in range(n_repeats):\n",
        "    print(f\"Running cross-validation #{repeat+1}\")\n",
        "    cv_skfold = StratifiedKFold(\n",
        "        n_splits=n_folds,\n",
        "        shuffle=True,\n",
        "        random_state=repeat,\n",
        "    )\n",
        "    cv_splits = cv_skfold.split(cam16_design_indices, y=cam16_design_labels)\n",
        "\n",
        "    for i, (train_indices, val_indices) in enumerate(cv_splits):\n",
        "        fold_start_time = datetime.now()\n",
        "\n",
        "        # Instantiate the TaskSampler for the current fold\n",
        "        task_sampler = TaskSampler(\n",
        "            features_dataset,\n",
        "            n_way=n_way,\n",
        "            n_shot=n_shots,\n",
        "            n_query=100,\n",
        "            n_tasks=10,\n",
        "        )\n",
        "\n",
        "        # Create data loaders using the TaskSampler\n",
        "        train_loader = DataLoader(\n",
        "            features_dataset,\n",
        "            batch_sampler=task_sampler,\n",
        "            num_workers=1,\n",
        "            pin_memory=True,\n",
        "            collate_fn=task_sampler.episodic_collate_fn,\n",
        "        )\n",
        "\n",
        "        # Instantiate the model\n",
        "        model = ModifiedPrototypicalNetworks(in_channels, num_classes)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch in train_loader:\n",
        "                # Extract support and query sets from the batch\n",
        "                support_images, support_labels, query_images, query_labels = batch\n",
        "\n",
        "                # Process support set to update prototypes\n",
        "                model.process_support_set(support_images, support_labels)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = model(query_images)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(logits, query_labels)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Validate the model\n",
        "            model.eval()\n",
        "            # Add validation code here if needed\n",
        "\n",
        "        fold_end_time = datetime.now()\n",
        "        fold_running_time = fold_end_time - fold_start_time\n",
        "        print(\"\\n-----------------------------Finished in {}---------------------------------------\\n\".format(fold_running_time))\n",
        "\n",
        "    # Clear output if needed\n",
        "    # clear_output()\n",
        "\n",
        "cv_end_time = datetime.now()\n",
        "cv_running_time = cv_end_time - cv_start_time\n",
        "print(\"\\nFinished cross-validation in {}\".format(cv_running_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "NPOXoP9zfxqa",
        "outputId": "58f9bc5f-4e75-456c-9739-1026d85c5dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FeaturesDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kather_emb_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFeaturesDataset\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_dataframe(kather_emb_df)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FeaturesDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We run a 5-fold cross-validation with 1 repeat (you can tweak these parameters)\n",
        "n_repeats = 1\n",
        "n_folds = 5\n",
        "train_metrics, val_metrics = [], []\n",
        "test_logits = []\n",
        "\n",
        "cv_start_time = datetime.now()\n",
        "\n",
        "for repeat in range(n_repeats):\n",
        "    print(f\"Running cross-validation #{repeat+1}\")\n",
        "    # We stratify with respect to the training labels\n",
        "    cv_skfold = StratifiedKFold(\n",
        "        n_splits=n_folds,\n",
        "        shuffle=True,\n",
        "        random_state=repeat,\n",
        "    )\n",
        "    cv_splits = cv_skfold.split(cam16_design_indices, y=cam16_design_labels)\n",
        "\n",
        "    # 1 training fold approximately takes 25 seconds\n",
        "    for i, (train_indices, val_indices) in enumerate(cv_splits):\n",
        "        fold_start_time = datetime.now()\n",
        "        trainer = TorchTrainer(\n",
        "            model=deepcopy(model2),\n",
        "            criterion=criterion,\n",
        "            metrics=metrics,\n",
        "            batch_size=16,                           # you can tweak this\n",
        "            num_epochs=15,                           # you can tweak this\n",
        "            learning_rate=1e-3,                      # you can tweak this\n",
        "            weight_decay=0.0,                        # you can tweak this\n",
        "            device=\"cuda:0\",\n",
        "            num_workers=multiprocessing.cpu_count(), # you can tweak this\n",
        "            optimizer=deepcopy(optimizer),\n",
        "            train_step=slide_level_train_step,\n",
        "            val_step=slide_level_val_step,\n",
        "            collator=pad_collate_fn,\n",
        "        )\n",
        "\n",
        "        print(f\"Running cross-validation on split #{i+1}\")\n",
        "        cam16_train_dataset = torch.utils.data.Subset(\n",
        "            cam16_design_dataset, indices=train_indices\n",
        "        )\n",
        "        cam16_val_dataset = torch.utils.data.Subset(\n",
        "            cam16_design_dataset, indices=val_indices\n",
        "        )\n",
        "        task_sampler = TaskSampler(\n",
        "                features_dataset,\n",
        "                n_way=n_way,\n",
        "                n_shot=n_shots,\n",
        "                n_query=100,\n",
        "                n_tasks=10,\n",
        "            )\n",
        "        features_loader = DataLoader(\n",
        "                features_dataset,\n",
        "                batch_sampler=task_sampler,\n",
        "                num_workers=1,\n",
        "                pin_memory=True,\n",
        "                collate_fn=task_sampler.episodic_collate_fn,\n",
        "            )\n",
        "\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "            # Training step for the given number of epochs\n",
        "            local_train_metrics, local_val_metrics = trainer.train(\n",
        "                cam16_train_dataset, cam16_val_dataset\n",
        "            )\n",
        "            # Predictions on test (logits, sigmoid(logits) = probability)\n",
        "            local_test_logits = trainer.predict(cam16_test_dataset)[1]\n",
        "\n",
        "        train_metrics.append(local_train_metrics)\n",
        "        val_metrics.append(local_val_metrics)\n",
        "        test_logits.append(local_test_logits)\n",
        "        fold_end_time = datetime.now()\n",
        "        fold_running_time = fold_end_time - fold_start_time\n",
        "        print(\"\\n-----------------------------Finished in {}---------------------------------------\\n\".format(fold_running_time))\n",
        "    #clear_output()\n",
        "cv_end_time = datetime.now()\n",
        "cv_running_time = cv_end_time - cv_start_time\n",
        "print(\"\\nFinished cross-validation in {}\".format(cv_running_time))"
      ],
      "metadata": {
        "id": "fqRdPjWBd1n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import get_cv_metrics, roc_auc_score\n",
        "\n",
        "cv_train_metrics = get_cv_metrics(train_metrics)\n",
        "cv_val_metrics = get_cv_metrics(val_metrics)\n",
        "test_metrics = trainer.evaluate(cam16_test_dataset)\n",
        "\n",
        "print(\"Cross-validation results:\")\n",
        "for k, v in cv_train_metrics.items():\n",
        "    print(f\"mean_train_{k}: {v}\")\n",
        "\n",
        "for k, v in cv_val_metrics.items():\n",
        "    print(f\"mean_val_{k}: {v}\")\n",
        "\n",
        "print(\"\\nEnsembling results on test set:\")\n",
        "test_auc = roc_auc_score(\n",
        "    cam16_test_dataset.labels,\n",
        "    np.mean(test_logits, axis=0)\n",
        ")\n",
        "print(f\"test_auc: {test_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPvk_Jl4N6FP",
        "outputId": "e4698e99-2757-45e4-91ed-ae23c4ff6ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation results:\n",
            "mean_train_auc: 0.9866 ± 0.0035\n",
            "mean_val_auc: 0.9564 ± 0.0318\n",
            "\n",
            "Ensembling results on test set:\n",
            "test_auc: 0.9235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import set_seed as set_seed_hf\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "dataset_name = \"/home/rakib/data/NCT-CRC-HE-100K-NONORM\"\n",
        "# You can change the dataset name above if you wish to finetune the model on your own dataset.\n",
        "\n",
        "\n",
        "# We set a seed globally for data loading and training\n",
        "SEED = 123\n",
        "\n",
        "def set_seed(seed: Optional[int] = None):\n",
        "    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n",
        "    When seed is None, disables deterministic mode.\n",
        "    Credits @BramVanroy\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        set_seed_hf(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "dataset = load_dataset(\"imagefolder\", data_dir=\"/home/rakib/data/NCT-CRC-HE-100K-NONORM\", cache_dir=cache_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "29edfc93239d406e95a3da2db64624f0",
            "6ac12503e90e407bb5a9e4624456d931",
            "7ecab4d44c2f4ce3b49a6b080751de69",
            "70b0e96a3dfe40eb9d9789ed423246ef",
            "365beeec3b6d41049cfe38e441d7de6f",
            "b49b5a9c7edf47ed8c2b153aa75ff862",
            "9d24fc263d3f4ec7bba577e35a9d1cb0",
            "bb263f16c75d4002a2b39a90d6313b7e",
            "b53fe3c76ae84f3fa58fe089f665eb55",
            "8d0b7ce792b54df0833026fa693ee546",
            "08ea6306e34845b5907a542739bdbec7",
            "c33a29f4048b43738268915e57254450",
            "082074ea8bc246078633084c64bb2ab2",
            "571fe119e0fd4fbdb530b5f71ae5fc01",
            "30af967379594e2f826c372fcdc2dd4e",
            "d81297c7374f4de2b1d0f710b1045664",
            "1896c4b3845d4e549c257776569f535a",
            "da6eea0c74c54efbaece76de6e82ed42",
            "6ca25b456519423594ce64562c1c285a",
            "136016af7f0b4a328117c2d82b2a4a0a",
            "1ca11877efc3453c9722475fb4c57d1a",
            "5049322b56454739bc1e6ae3e333617f"
          ]
        },
        "id": "cRA5OJpPOmk9",
        "outputId": "b7ebcbbe-907b-4946-9242-2d8014791eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/100000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29edfc93239d406e95a3da2db64624f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c33a29f4048b43738268915e57254450"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug dataset properties\n",
        "print(dataset.keys())\n",
        "print(dataset.items())\n",
        "print(dataset.unique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbtPnsmnH8jb",
        "outputId": "8fe183d4-fbbf-420c-a0b1-bb4ebb718abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['train'])\n",
            "dict_items([('train', Dataset({\n",
            "    features: ['image', 'label'],\n",
            "    num_rows: 100000\n",
            "}))])\n",
            "<bound method DatasetDict.unique of DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image', 'label'],\n",
            "        num_rows: 100000\n",
            "    })\n",
            "})>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nct_data = dataset['train']\n",
        "\n",
        "# Get labels and images\n",
        "labels = nct_data['label']\n",
        "images = nct_data['image']"
      ],
      "metadata": {
        "id": "RnHxU04FSovQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT NECESSARY IF PREVIOUS SLIDES WERE SEQUENTIALLY EXECUTED\n",
        "\n",
        "import warnings\n",
        "from copy import deepcopy\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from trainer import TorchTrainer, slide_level_train_step, slide_level_val_step"
      ],
      "metadata": {
        "id": "bbkAwDPX4HI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This strategy doesnot take into account the class imbalance issue\n",
        "\n",
        "# Define the number of samples you want to randomly select\n",
        "num_samples = 1000  # Change this number to your desired value\n",
        "\n",
        "# Randomly sample from the dataset according to the number of samples\n",
        "random_indices = random.sample(range(len(labels)), num_samples)\n",
        "\n",
        "# Extract sampled labels and images\n",
        "sampled_labels = [labels[i] for i in random_indices]\n",
        "sampled_images = [images[i] for i in random_indices]\n",
        "\n",
        "# Strategy to solve the class imbalance issue\n",
        "'''\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define the number of samples you want to randomly select\n",
        "num_samples_per_class = 10  # Change this number to your desired value per class\n",
        "\n",
        "# Initialize a dictionary to store sampled indices for each class\n",
        "class_indices = defaultdict(list)\n",
        "\n",
        "# Map class names to class labels\n",
        "class_name_to_label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "# Collect indices for each class\n",
        "for i, label in enumerate(labels):\n",
        "    class_name = label2id[label]\n",
        "    class_label = class_name_to_label[class_name]\n",
        "    class_indices[class_label].append(i)\n",
        "\n",
        "# Randomly sample from each class\n",
        "sampled_indices = []\n",
        "for class_label, indices in class_indices.items():\n",
        "    sampled_indices.extend(random.sample(indices, num_samples_per_class))\n",
        "\n",
        "# Extract sampled labels and images\n",
        "sampled_labels = [labels[i] for i in sampled_indices]\n",
        "sampled_images = [images[i] for i in sampled_indices]\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dir3QEVuV8Np",
        "outputId": "f6d0f7fc-5db5-4950-afb7-97d24558e24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom collections import defaultdict\\n\\n# Define the number of samples you want to randomly select\\nnum_samples_per_class = 10  # Change this number to your desired value per class\\n\\n# Initialize a dictionary to store sampled indices for each class\\nclass_indices = defaultdict(list)\\n\\n# Map class names to class labels\\nclass_name_to_label = {v: k for k, v in label2id.items()}\\n\\n# Collect indices for each class\\nfor i, label in enumerate(labels):\\n    class_name = label2id[label]\\n    class_label = class_name_to_label[class_name]\\n    class_indices[class_label].append(i)\\n\\n# Randomly sample from each class\\nsampled_indices = []\\nfor class_label, indices in class_indices.items():\\n    sampled_indices.extend(random.sample(indices, num_samples_per_class))\\n\\n# Extract sampled labels and images\\nsampled_labels = [labels[i] for i in sampled_indices]\\nsampled_images = [images[i] for i in sampled_indices]\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the percentage for the validation set\n",
        "split_percentage = 0.5\n",
        "\n",
        "# Split the sampled data into train and validation sets\n",
        "train_labels, val_labels, train_images, val_images = train_test_split(sampled_labels, sampled_images, test_size=split_percentage)\n",
        "\n",
        "# Strategy to solve the class imbalance issue\n",
        "'''\n",
        "# Split the sampled data into train and validation sets\n",
        "train_indices, val_indices = train_test_split(sampled_indices, test_size=split_percentage, stratify=sampled_labels)\n",
        "\n",
        "# Because it's a list function, this step is necessary:\n",
        "\n",
        "# Extract labels and images for train and validation sets\n",
        "train_labels = [labels[i] for i in train_indices]\n",
        "train_images = [images[i] for i in train_indices]\n",
        "\n",
        "val_labels = [labels[i] for i in val_indices]\n",
        "val_images = [images[i] for i in val_indices]\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTZLSMPPIXqe",
        "outputId": "68f490d6-88c3-42d0-8c84-7abfbf05e1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Split the sampled data into train and validation sets\\ntrain_indices, val_indices = train_test_split(sampled_indices, test_size=split_percentage, stratify=sampled_labels)\\n\\n# Because it's a list function, this step is necessary:\\n\\n# Extract labels and images for train and validation sets\\ntrain_labels = [labels[i] for i in train_indices]\\ntrain_images = [images[i] for i in train_indices]\\n\\nval_labels = [labels[i] for i in val_indices]\\nval_images = [images[i] for i in val_indices]\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From the NCT-CRC 999 samples, we create train and validation sets of 500 images each\n",
        "\n",
        "# test_dataset_path = \"/home/rakib/data/CRC-VAL-HE-7K\"\n",
        "\n",
        "# Test dataset contains 7,180 images\n",
        "test_dataset = load_dataset(\"imagefolder\", data_dir=\"/home/rakib/data/CRC-VAL-HE-7K\", cache_dir=cache_dir)\n",
        "\n",
        "t_data =  test_dataset['train']\n",
        "\n",
        "# Get labels and images\n",
        "t_labels = t_data['label']\n",
        "t_images = t_data['image']\n",
        "\n",
        "# Randomly sample from the dataset according to the number of samples\n",
        "random_t_indices = random.sample(range(len(t_labels)), num_samples)\n",
        "\n",
        "# Extract sampled labels and images\n",
        "sampled_t_labels = [t_labels[i] for i in random_t_indices]\n",
        "sampled_t_images = [t_images[i] for i in random_t_indices]"
      ],
      "metadata": {
        "id": "mQmdnZgjIg9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "087f54183e3d455fbecb666a1d1d2791",
            "707d28b3a0954860b1821cea0a7b7510",
            "c5856aa6887a448db894fc59941e65bb",
            "b681f186b3a64b93857aaeb0e03486dd",
            "9f054e5338394dd9b09f07a9d73dbff5",
            "b36dfc5e526a4d73b51f1f0466544192",
            "7c2e3eadfc0e4ca49708cf554322c0a6",
            "026808b2c3764fe4ad58d9b69fe7e123",
            "99263ef5ab5445a48881c6be9c7b1e96",
            "b43351df9b4f4423ad665afbd812fb53",
            "5b91ce2fd0ec45be971b3dc910e0a896",
            "002f874bab8d4b109bd10717b29f1956",
            "f2bcd5697e6f4635ac4d6022bcf00ebf",
            "ec9ff6a986324fe2993d100b254d40f9",
            "531e232971cf4f4d8404ddf86583ed01",
            "f0d9730add6b4172bdfdf50880c7d7ee",
            "a20a8b67111d49aaad7f0b2cca3ba979",
            "8d5bf3073e59416f94161166fa58b4f9",
            "00d20cf24fd74347b240e79b289d2b2b",
            "8811ba5e28c241b0b6393be4d0f683b9",
            "371f2d4dd80c44108e01368703b17c90",
            "c0faff2b2345442bb3429b0712b51875"
          ]
        },
        "outputId": "8f60a650-59cc-4a14-dd89-cbb30469fa00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/7180 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "087f54183e3d455fbecb666a1d1d2791"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "002f874bab8d4b109bd10717b29f1956"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and validation datasets\n",
        "train_dataset = {'image': train_images, 'label': train_labels}\n",
        "val_dataset = {'image': val_images, 'label': val_labels}\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(f\"Number of samples in the train set: {len(train_labels)}\")\n",
        "print(f\"Number of samples in the validation set: {len(val_labels)}\")\n",
        "\n",
        "subset_dataset = {'image': sampled_t_images, 'label': sampled_t_labels}\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\\n\" f\"Validation dataset size: {len(val_dataset)}\\n\" f\"Test dataset size: {len(test_dataset)}\\n\")"
      ],
      "metadata": {
        "id": "f_QmHXK0S-hD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb171066-37ef-4978-cc5f-f67eabb0949a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the train set: 500\n",
            "Number of samples in the validation set: 500\n",
            "Training dataset size: 2\n",
            "Validation dataset size: 2\n",
            "Test dataset size: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.unique"
      ],
      "metadata": {
        "id": "ocAh5ZqIMliv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab62a8e9-bfd9-4bc2-d1ed-2a24966ee2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DatasetDict.unique of DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 7180\n",
              "    })\n",
              "})>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "wvDOCSD96k2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_dict(train_dataset)"
      ],
      "metadata": {
        "id": "arXzcTka6nth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create train and validation datasets\n",
        "#train_dataset = Dataset.from_dict({'image': train_images, 'label': train_labels})\n",
        "val_dataset = Dataset.from_dict(val_dataset)"
      ],
      "metadata": {
        "id": "hyORDvxGUB1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Dataset.from_dict({'image': t_data['image'], 'label': t_data['label']})\n",
        "#subset_dataset = Dataset.from_dict(\"image\": [data[\"image\"] for data in balanced_test_dataset], \"label\": [data[\"label\"] for data in balanced_test_dataset])\n",
        "print(f\"Training dataset size: {len(train_dataset)}\\n\" f\"Validation dataset size: {len(val_dataset)}\\n\" f\"Test dataset size: {len(test_dataset)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg59SaZVUCyU",
        "outputId": "8e662846-0e05-42e8-d6d0-96d24d390246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 500\n",
            "Validation dataset size: 500\n",
            "Test dataset size: 7180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_dataset = Dataset.from_dict(subset_dataset)"
      ],
      "metadata": {
        "id": "OO7kwvXn5_IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print subset dataset properties\n",
        "\n",
        "print(subset_dataset.unique)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1GN632n7K10",
        "outputId": "13295905-27e7-4475-ad86-e54e45f80fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Dataset.unique of Dataset({\n",
            "    features: ['image', 'label'],\n",
            "    num_rows: 1000\n",
            "})>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to know number of elements for each class in the subset_dataset\n",
        "\n",
        "from collections import Counter\n",
        "class_counts = Counter(subset_dataset['label'])\n",
        "print(class_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqf-o4Z-7kos",
        "outputId": "4a10e2c2-9d4a-44e3-8cd8-20543fb5bcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 179, 8: 170, 4: 144, 1: 132, 6: 118, 3: 81, 5: 76, 7: 59, 2: 41})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor = AutoImageProcessor.from_pretrained(\"owkin/phikon\")\n",
        "print(image_processor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dX39lHgvTuV",
        "outputId": "e0df8278-aff1-4a09-bbcc-e2ded310fc14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViTImageProcessor {\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.485,\n",
            "    0.456,\n",
            "    0.406\n",
            "  ],\n",
            "  \"image_processor_type\": \"ViTImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.229,\n",
            "    0.224,\n",
            "    0.225\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 224,\n",
            "    \"width\": 224\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "# ImageNet normalization\n",
        "normalize = Normalize(\n",
        "    mean=image_processor.image_mean,\n",
        "    std=image_processor.image_std\n",
        ")\n",
        "\n",
        "# train transforms = random crop, resizing to 224x224, random flip, normalization\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# val transforms = resizing to 224x224, normalization\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        Resize(image_processor.size[\"height\"]),\n",
        "        CenterCrop(image_processor.size[\"height\"]),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "8uYOk70xvknM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "def preprocess_train(example_batch: dict[str, Any]) -> dict[str, Any]:\n",
        "    \"\"\"Apply ``train_transforms`` across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        train_transforms(image) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "\n",
        "def preprocess_val(example_batch: dict[str, Any]) -> dict[str, Any]:\n",
        "    \"\"\"Apply ``val_transforms`` across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        val_transforms(image) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "'''\n",
        "\n",
        "# Modified to avoid type error due to python3.8\n",
        "def preprocess_train(example_batch: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Apply ``train_transforms`` across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        train_transforms(image) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "\n",
        "def preprocess_val(example_batch: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Apply ``val_transforms`` across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        val_transforms(image) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "# Apply the transformations\n",
        "train_dataset.set_transform(preprocess_train)\n",
        "val_dataset.set_transform(preprocess_val)\n",
        "test_dataset.set_transform(preprocess_val)"
      ],
      "metadata": {
        "id": "TevxBST8v7kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBZaD4f0iRPF",
        "outputId": "4219af9b-395f-4ffb-e4a1-897c32c1c187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetInfo(description='', citation='', homepage='', license='', features={'image': Image(decode=True, id=None), 'label': Value(dtype='int64', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, dataset_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "# Labels from our dataset\n",
        "label2id = {\n",
        "    '0': \"ADI\",\n",
        "    '1': \"BACK\",\n",
        "    '2': \"DEB\",\n",
        "    '3': \"LYM\",\n",
        "    '4': \"MUC\",\n",
        "    '5': \"MUS\",\n",
        "    '6': \"NORM\",\n",
        "    '7': \"STR\",\n",
        "    '8': \"TUM\"\n",
        "}\n",
        "id2label = {v: k for (k, v) in label2id.items()}\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"owkin/phikon\",\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=False,\n",
        "    cache_dir=cache_dir,\n",
        ")\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "A4RaTMFQyc-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bcef4c8-cacb-4ed1-a21c-9d9aba9a1e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at owkin/phikon and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 85805577 || all params: 85805577 || trainable%: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We also create a version of Phikon where the model is kept frozen and only the classifier head is trained (0.01% of the training parameters).\n",
        "from copy import deepcopy\n",
        "\n",
        "frozen_model = deepcopy(model)\n",
        "\n",
        "for name, param in frozen_model.named_parameters():\n",
        "     if not name.startswith(\"classifier.\"):\n",
        "        param.requires_grad = False\n",
        "print_trainable_parameters(frozen_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9rR_rtCyy2y",
        "outputId": "2799b9d5-c02a-483e-ac8a-9361fa566297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 6921 || all params: 85805577 || trainable%: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA fine-tuning only requires 0.70% of the original trainable parameters!\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "# load and configure LoRA from Hugging Face peft library\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"classifier\"],\n",
        ")\n",
        "lora_model = get_peft_model(model, config)\n",
        "print_trainable_parameters(lora_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69YnZzFzzI1Z",
        "outputId": "66e5e6ef-b63a-4d8d-b59c-0a950ce360dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 596745 || all params: 86402322 || trainable%: 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Config.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# LoRA configuration\n",
        "\n",
        "batch_size = 24\n",
        "args = TrainingArguments(\n",
        "    \"phikon-finetuned-nct-1k\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-3,\n",
        "    gradient_accumulation_steps=1,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True,\n",
        "    seed=SEED,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",  # dataset is roughly balanced\n",
        "    push_to_hub=False,\n",
        "    label_names=[\"labels\"],\n",
        ")\n",
        "\n",
        "# Metric configuration\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred: np.ndarray) -> float:\n",
        "    \"\"\"Computes accuracy on a batch of predictions.\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "# Inputs generation for training\n",
        "\n",
        "# Modified to avoid type error (def collate_fn(examples) -> dict[str, torch.Tensor]:)\n",
        "def collate_fn(examples) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Create the inputs for LoRA from an example in the dataset.\"\"\"\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# Here is the final trainer\n",
        "trainer_lora = Trainer(\n",
        "    model=lora_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf925P7EzN3Y",
        "outputId": "6761451d-0e46-4d5f-bb5d-44d591a4e71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "\n",
        "import warnings\n",
        "\n",
        "from transformers.utils import logging\n",
        "\n",
        "\n",
        "# We display the accuracy on the test set at the end\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    train_results_lora = trainer_lora.train()\n",
        "    metrics_lora = trainer_lora.evaluate(test_dataset)\n",
        "    trainer_lora.log_metrics(\"Fine-tuned model: VAL-CRC-7K\", metrics_lora)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "FANrihF4zw2_",
        "outputId": "fbae5cbc-aed1-430e-8345-8bf9a4a90251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [210/210 00:36, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.249800</td>\n",
              "      <td>0.232411</td>\n",
              "      <td>0.954000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.139000</td>\n",
              "      <td>0.197188</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.169068</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.044000</td>\n",
              "      <td>0.237637</td>\n",
              "      <td>0.970000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.311000</td>\n",
              "      <td>0.219136</td>\n",
              "      <td>0.968000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.337700</td>\n",
              "      <td>0.211805</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.320900</td>\n",
              "      <td>0.227102</td>\n",
              "      <td>0.966000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.010600</td>\n",
              "      <td>0.257810</td>\n",
              "      <td>0.964000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.218674</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.213999</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-21 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-42 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-63 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-84 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-105 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-126 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-147 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-168 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-189 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-210 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:15]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Fine-tuned model: VAL-CRC-7K metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_accuracy           =     0.8118\n",
            "  eval_loss               =     0.9193\n",
            "  eval_runtime            = 0:00:15.10\n",
            "  eval_samples_per_second =    475.193\n",
            "  eval_steps_per_second   =     19.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We now do the same training thing fully-frozen Phikon.\n",
        "\n",
        "We observe up to a +2 increase in multi-class accuracy using LoRA fine-tuning, for only 30 seconds of extra training cost.\n"
      ],
      "metadata": {
        "id": "FMFWEaC86k_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer_frozen = Trainer(\n",
        "    frozen_model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "# Evaluation on test_dataset\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    train_results_frozen = trainer_frozen.train()\n",
        "    metrics_frozen = trainer_frozen.evaluate(test_dataset)\n",
        "    trainer_frozen.log_metrics(\"Frozen model: VAL-CRC-7K\", metrics_frozen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "pmrFmOx36Yfy",
        "outputId": "9f5c48d1-2ea1-4821-84aa-db036e31d278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [210/210 00:25, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.124800</td>\n",
              "      <td>0.173862</td>\n",
              "      <td>0.966000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.010600</td>\n",
              "      <td>0.193067</td>\n",
              "      <td>0.968000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.224724</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.120400</td>\n",
              "      <td>0.244578</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.229637</td>\n",
              "      <td>0.964000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.608700</td>\n",
              "      <td>0.191229</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.355300</td>\n",
              "      <td>0.200367</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.222482</td>\n",
              "      <td>0.968000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.214440</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.211076</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-21 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-42 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-63 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-84 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-105 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-126 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-147 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-168 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-189 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory phikon-finetuned-nct-1k/checkpoint-210 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Frozen model: VAL-CRC-7K metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_accuracy           =     0.8507\n",
            "  eval_loss               =     0.5789\n",
            "  eval_runtime            = 0:00:21.96\n",
            "  eval_samples_per_second =    326.948\n",
            "  eval_steps_per_second   =     13.661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizing features**\n",
        "\n",
        "Code to visualize the features. This part helps differentiate between a frozen model and LoRA in order to examine the differences in the embeddings."
      ],
      "metadata": {
        "id": "n_yPmdQv6yhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib.axes._axes import Axes\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# First we define a set of functions to\n",
        "# 1) get the embeddings from the models\n",
        "# 2) compute the 2D projections using the t-SNE algorithm\n",
        "# 3) visualize these projections using ``seaborn```\n",
        "\n",
        "def get_raw_embeddings(model, dataset, use_fp16: bool = True):\n",
        "    \"\"\"Retrieve tiles embeddings from a model equipped with a classifier head.\"\"\"\n",
        "    embeddings = []\n",
        "    for pixel_values in tqdm(dataset[\"pixel_values\"]):\n",
        "        image = pixel_values.unsqueeze(0).to(\n",
        "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "            torch.float16 if use_fp16 else torch.float32\n",
        "          )\n",
        "        output = model(image, output_hidden_states=True)\n",
        "        _embeddings = output.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n",
        "        embeddings.append(_embeddings)\n",
        "    return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "\n",
        "def get_tsne_embeddings(raw_embeddings: np.ndarray, **kwargs):\n",
        "    \"\"\"Compute 2-dimensional tsne projections from raw embeddings.\"\"\"\n",
        "    tsne = TSNE(**kwargs)\n",
        "    tsne_embeddings = tsne.fit_transform(raw_embeddings)\n",
        "    tsne_embeddings = pd.DataFrame(tsne_embeddings, columns=[\"tsne-1\", \"tsne-2\"])\n",
        "    tsne_embeddings[\"Tissue type\"] = test_subset_labels\n",
        "    tsne_embeddings[\"Tissue type\"] = tsne_embeddings[\"Tissue type\"].astype(str).replace(label2id)\n",
        "    return tsne_embeddings\n",
        "\n",
        "def plot_tsne_embeddings(tsne_embeddings: np.ndarray, title: str, ax: Axes):\n",
        "    \"\"\"Plot tsne embeddings in the 2D space.\"\"\"\n",
        "    sns.scatterplot(\n",
        "        x=\"tsne-1\", y=\"tsne-2\",\n",
        "        hue=\"Tissue type\",\n",
        "        palette=sns.color_palette(\"hls\", 9),\n",
        "        data=tsne_embeddings,\n",
        "        legend=\"full\",\n",
        "        alpha=0.3,\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(title)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "1VxVSXCS6tMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ve_ScMaFIu-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A subset of 1,000 images from the original test set is considered for the inference."
      ],
      "metadata": {
        "id": "Tb-Z8VFC7uMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_size = 1000\n",
        "test_subset = test_dataset[:subset_size]\n",
        "test_subset_labels = np.array(test_subset[\"label\"])\n",
        "\n",
        "print(f\"Computing LORA and frozen models embeddings on {subset_size} test images...\")\n",
        "test_subset_embeddings_lora = get_raw_embeddings(\n",
        "    model=lora_model, dataset=test_subset\n",
        ")\n",
        "test_subset_embeddings_frozen = get_raw_embeddings(\n",
        "    model=frozen_model, dataset=test_subset\n",
        ")\n",
        "\n",
        "print(\"Computing tsne projections...\")\n",
        "tsne_embeddings_lora = get_tsne_embeddings(\n",
        "    test_subset_embeddings_lora, n_components=2\n",
        ")\n",
        "tsne_embeddings_frozen = get_tsne_embeddings(\n",
        "    test_subset_embeddings_frozen, n_components=2\n",
        "  )"
      ],
      "metadata": {
        "id": "3U77O09K7jtq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "ee095701-278b-4d3d-9c05-abaf2ce4f9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m subset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m test_subset \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m[:subset_size]\n\u001b[1;32m      3\u001b[0m test_subset_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_subset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing LORA and frozen models embeddings on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The differences between the LoRA fine-tuned and frozen models are small due to the highly separable nature of NCT-CRC prediction task (different tissues can be distinguished easily by the naked eye). However, we notice that LoRA fine-tuning allows to better disentangle clusters such as Lymphocytes (Yellow) and Tumor (red), which can play a significant role in cancer diagnosis."
      ],
      "metadata": {
        "id": "5xJCjl4575wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Plotting in 2 dimensions.\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "axes[0] = plot_tsne_embeddings(\n",
        "    tsne_embeddings_lora, title=\"Lora embeddings\", ax=axes[0]\n",
        ")\n",
        "axes[1] = plot_tsne_embeddings(\n",
        "    tsne_embeddings_frozen, title=\"Frozen embeddings\", ax=axes[1]\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iJ1gxNxc7ysh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDDqFK8q73U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: There are 9 unique elements in train_dataset['label']. I want to know number of elements for each unique element\n",
        "\n",
        "unique_labels, counts = np.unique(train_dataset['label'], return_counts=True)\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"Label: {label}, Count: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvTxPljxqL9y",
        "outputId": "e5279168-b289-4f5a-815c-7a5d86352770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 0, Count: 62\n",
            "Label: 1, Count: 60\n",
            "Label: 2, Count: 54\n",
            "Label: 3, Count: 51\n",
            "Label: 4, Count: 52\n",
            "Label: 5, Count: 60\n",
            "Label: 6, Count: 52\n",
            "Label: 7, Count: 43\n",
            "Label: 8, Count: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Get the unique labels and their counts\n",
        "unique_labels, counts = np.unique(test_dataset['train']['label'], return_counts=True)"
      ],
      "metadata": {
        "id": "0Ch2oLmMbiX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store the indices of each class\n",
        "class_indices = {}\n",
        "for i, label in enumerate(test_dataset['train']['label']):\n",
        "    if label not in class_indices:\n",
        "        class_indices[label] = []\n",
        "    class_indices[label].append(i)"
      ],
      "metadata": {
        "id": "1yRd6nPTbmdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a balanced subset of the test dataset\n",
        "balanced_test_dataset = []"
      ],
      "metadata": {
        "id": "H1hRCDnGb2m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the indices\n",
        "all_sampled_indices = []\n",
        "\n",
        "for label in unique_labels:\n",
        "    # Randomly select 25 samples from each class\n",
        "    sampled_indices = random.sample(class_indices[label], 10)\n",
        "    # Add the sampled indices to the overall list\n",
        "    all_sampled_indices.extend(sampled_indices)\n"
      ],
      "metadata": {
        "id": "vMaX6IGLb6Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_sampled_indices))\n",
        "print(np.unique(all_sampled_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPu6TXahgQQJ",
        "outputId": "41cd9a05-e5d3-4a14-ef8f-75ced0bcf81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "[ 199  208  215  413  471  886  970 1101 1104 1170 1351 1379 1507 1533\n",
            " 1786 1855 1880 1893 1951 2081 2192 2218 2238 2307 2377 2409 2411 2439\n",
            " 2459 2480 2542 2624 2654 2702 2792 2825 2869 2886 2912 3151 3182 3327\n",
            " 3408 3412 3728 3831 3892 4126 4162 4172 4295 4365 4368 4381 4408 4515\n",
            " 4630 4647 4701 4703 4902 4945 5005 5028 5032 5086 5100 5216 5247 5424\n",
            " 5557 5607 5637 5656 5657 5659 5765 5805 5817 5867 6221 6245 6271 6513\n",
            " 6573 6627 6802 6863 6895 6983]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the sampled images and labels to the balanced subset\n",
        "for index in all_sampled_indices:\n",
        "    balanced_test_dataset.append({'image': test_dataset['train']['image'][index], 'label': test_dataset['train']['label'][index]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hWDQCIjNmbCf",
        "outputId": "eb4d10d3-7265-41e3-c645-21a685d11e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add the sampled images and labels to the balanced subset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m all_sampled_indices:\n\u001b[0;32m----> 3\u001b[0m     balanced_test_dataset\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[index], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: test_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][index]})\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/arrow_dataset.py:2785\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:442\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    441\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_column(pa_table)\n\u001b[0;32m--> 442\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:218\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_column\u001b[0;34m(self, column, column_name)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m column\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/features.py:1951\u001b[0m, in \u001b[0;36mFeatures.decode_column\u001b[0;34m(self, column, column_name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \n\u001b[1;32m   1941\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;124;03m        `list[Any]`\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1951\u001b[0m         [decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m   1952\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1953\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   1954\u001b[0m     )\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/features.py:1951\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \n\u001b[1;32m   1941\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;124;03m        `list[Any]`\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1951\u001b[0m         [\u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m   1952\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1953\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   1954\u001b[0m     )\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/features.py:1339\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[0;32m-> 1339\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/image.py:185\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[0;32m--> 185\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:1209\u001b[0m, in \u001b[0;36mTiffImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_load_libtiff:\n\u001b[0;32m-> 1209\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_libtiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mload()\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:1296\u001b[0m, in \u001b[0;36mTiffImageFile._load_libtiff\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m# 4 bytes, otherwise the trace might error out\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m     n, err \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfpfp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;66;03m# we have something else.\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have fileno or getvalue. just reading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "#subset_dataset = Dataset.from_dict(\"image\": [data[\"image\"] for data in balanced_test_dataset], \"label\": [data[\"label\"] for data in balanced_test_dataset])\n",
        "#subset_dataset = Dataset.from_dict({'image': balanced_test_dataset[\"image\"], \"label\": balanced_test_dataset[\"label\"]})\n",
        "\n",
        "subset_dataset = Dataset.from_dict({'image': test_dataset['train']['image'][index] for index in all_sampled_indices, 'label': test_dataset['train']['label'][index] for index in all_sampled_indices})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "0ARIupkvgrsj",
        "outputId": "2326e6d0-733d-4ef4-e2fe-5b8126cb1939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3909063921.py, line 5)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[40], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    subset_dataset = Dataset.from_dict({'image': test_dataset['train']['image'][index] for index in all_sampled_indices, 'label': test_dataset['train']['label'][index] for index in all_sampled_indices})\u001b[0m\n\u001b[0m                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: The above cell is taking too long to run. Help me Parallelize the processing: The cell is currently processing the data sequentially. You can try parallelizing the processing to see if it improves the performance.\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "def process_data(data):\n",
        "    # Perform data processing on each data point\n",
        "    processed_data = ...\n",
        "    return processed_data\n",
        "\n",
        "# Create a pool of worker processes\n",
        "pool = mp.Pool(processes=mp.cpu_count())\n",
        "\n",
        "# Use the pool to process the data in parallel\n",
        "processed_data = pool.map(process_data, dataset)\n",
        "\n",
        "# Close the pool\n",
        "pool.close()\n",
        "\n",
        "# Combine the processed data into a new dataset\n",
        "processed_dataset = Dataset.from_dict(processed_data)\n"
      ],
      "metadata": {
        "id": "dJLZnKEech4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: There are 9 classes in test dataset. I want to make a balanced subset of test_dataset, containing 100 random samples from each class.\n",
        "\n",
        "import random\n",
        "\n",
        "# Get the unique labels and their counts\n",
        "unique_labels, counts = np.unique(test_dataset['train']['label'], return_counts=True)\n",
        "\n",
        "# Initialize a dictionary to store the indices of each class\n",
        "class_indices = {}\n",
        "for i, label in enumerate(test_dataset['train']['label']):\n",
        "    if label not in class_indices:\n",
        "        class_indices[label] = []\n",
        "    class_indices[label].append(i)\n",
        "\n",
        "# Create a balanced subset of the test dataset\n",
        "balanced_test_dataset = []\n",
        "for label in unique_labels:\n",
        "    # Randomly select 100 samples from each class\n",
        "    sampled_indices = random.sample(class_indices[label], 100)\n",
        "    # Add the sampled images and labels to the balanced subset\n",
        "    for index in sampled_indices:\n",
        "        balanced_test_dataset.append({'image': test_dataset['train']['image'][index], 'label': test_dataset['train']['label'][index]})\n",
        "\n",
        "# Print the size of the balanced test dataset\n",
        "print(f\"Balanced test dataset size: {len(balanced_test_dataset)}\")\n",
        "'''\n",
        "from datasets import Dataset\n",
        "balanced_test_dict = {\n",
        "    \"image\": [data[\"image\"] for data in balanced_test_dataset],\n",
        "    \"label\": [data[\"label\"] for data in balanced_test_dataset],\n",
        "}\n",
        "balanced_test_dataset = Dataset.from_dict(balanced_test_dict)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q0kS7a29r2zi",
        "outputId": "4bf1e921-7a49-47cc-c278-47cdb9889f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Add the sampled images and labels to the balanced subset\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m sampled_indices:\n\u001b[0;32m---> 22\u001b[0m         balanced_test_dataset\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[index], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: test_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][index]})\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print the size of the balanced test dataset\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalanced test dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(balanced_test_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/arrow_dataset.py:2785\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:442\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    441\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_column(pa_table)\n\u001b[0;32m--> 442\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/formatting/formatting.py:218\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_column\u001b[0;34m(self, column, column_name)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m column\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/features.py:1951\u001b[0m, in \u001b[0;36mFeatures.decode_column\u001b[0;34m(self, column, column_name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \n\u001b[1;32m   1941\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;124;03m        `list[Any]`\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1951\u001b[0m         [decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m   1952\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1953\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   1954\u001b[0m     )\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/features.py:1951\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \n\u001b[1;32m   1941\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;124;03m        `list[Any]`\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1951\u001b[0m         [\u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m   1952\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1953\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   1954\u001b[0m     )\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/features.py:1339\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[0;32m-> 1339\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/datasets/features/image.py:166\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_local_path(path):\n\u001b[0;32m--> 166\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m         source_url \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/Image.py:3288\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3285\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3288\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[1;32m   3291\u001b[0m     checked_formats \u001b[38;5;241m=\u001b[39m formats\u001b[38;5;241m.\u001b[39mcopy()\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/Image.py:3274\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3272\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   3273\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3274\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3275\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   3276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:1082\u001b[0m, in \u001b[0;36mTiffImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Legacy tag entries \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/ImageFile.py:137\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;167;01mIndexError\u001b[39;00m,  \u001b[38;5;66;03m# end of data\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;167;01mTypeError\u001b[39;00m,  \u001b[38;5;66;03m# end of data (ord)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m         struct\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    144\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(v) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mv\u001b[39;00m\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:1109\u001b[0m, in \u001b[0;36mTiffImageFile._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- ifh: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(ifh))  \u001b[38;5;66;03m# Use repr to avoid str(bytes)\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# and load the first frame\u001b[39;00m\n\u001b[0;32m-> 1109\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seek\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:1171\u001b[0m, in \u001b[0;36mTiffImageFile._seek\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mifd \u001b[38;5;241m=\u001b[39m ImageFileDirectory_v1\u001b[38;5;241m.\u001b[39mfrom_v2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_v2)\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__frame \u001b[38;5;241m=\u001b[39m frame\n\u001b[0;32m-> 1171\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:1420\u001b[0m, in \u001b[0;36mTiffImageFile._setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- pil mode: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression\n\u001b[0;32m-> 1420\u001b[0m xres \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_RESOLUTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m yres \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_v2\u001b[38;5;241m.\u001b[39mget(Y_RESOLUTION, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xres \u001b[38;5;129;01mand\u001b[39;00m yres:\n",
            "File \u001b[0;32m/usr/lib/python3.8/_collections_abc.py:660\u001b[0m, in \u001b[0;36mMapping.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:571\u001b[0m, in \u001b[0;36mImageFileDirectory_v2.__getitem__\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    569\u001b[0m     typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagtype[tag]\n\u001b[1;32m    570\u001b[0m     size, handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_dispatch[typ]\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28mself\u001b[39m[tag] \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_api\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# check type\u001b[39;00m\n\u001b[1;32m    572\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tags_v2[tag]\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_api \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:762\u001b[0m, in \u001b[0;36mImageFileDirectory_v2.load_rational\u001b[0;34m(self, data, legacy_api)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine\u001b[39m(a, b):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b) \u001b[38;5;28;01mif\u001b[39;00m legacy_api \u001b[38;5;28;01melse\u001b[39;00m IFDRational(a, b)\n\u001b[0;32m--> 762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/project-6/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:762\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine\u001b[39m(a, b):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b) \u001b[38;5;28;01mif\u001b[39;00m legacy_api \u001b[38;5;28;01melse\u001b[39;00m IFDRational(a, b)\n\u001b[0;32m--> 762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(combine(num, denom) \u001b[38;5;28;01mfor\u001b[39;00m num, denom \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(vals[::\u001b[38;5;241m2\u001b[39m], vals[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to know number of elements for each class in the balanced_test_dataset\n",
        "\n",
        "# Get the unique labels and their counts in the balanced test dataset\n",
        "unique_labels, counts = np.unique([data['label'] for data in balanced_test_dataset], return_counts=True)\n",
        "\n",
        "# Print the number of elements for each class\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"Label: {label}, Count: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KUoj07bfY5Z",
        "outputId": "06b1141d-0ada-4f3e-d603-fd8147f63e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 0, Count: 100\n",
            "Label: 1, Count: 100\n",
            "Label: 2, Count: 100\n",
            "Label: 3, Count: 100\n",
            "Label: 4, Count: 100\n",
            "Label: 5, Count: 100\n",
            "Label: 6, Count: 100\n",
            "Label: 7, Count: 100\n",
            "Label: 8, Count: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "balanced_test_dict = {\n",
        "    \"image\": [data[\"image\"] for data in balanced_test_dataset],\n",
        "    \"label\": [data[\"label\"] for data in balanced_test_dataset],\n",
        "}\n",
        "balanced_test_dataset = Dataset.from_dict(balanced_test_dict)"
      ],
      "metadata": {
        "id": "rTL-cwUPft8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "6ae3dcfa-1e67-4bca-de52-281c38a90483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'balanced_test_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      2\u001b[0m balanced_test_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbalanced_test_dataset\u001b[49m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m balanced_test_dataset],\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      6\u001b[0m balanced_test_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(balanced_test_dict)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'balanced_test_dataset' is not defined"
          ]
        }
      ]
    }
  ]
}